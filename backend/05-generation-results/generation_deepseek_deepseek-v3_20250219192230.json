{
  "query": "What is GRPO",
  "timestamp": "2025-02-19T19:22:30.127626",
  "provider": "deepseek",
  "model": "deepseek-v3",
  "response": "GRPO stands for **Group Relative Policy Optimization**, a method introduced by Shao et al. in 2024 to reduce the training costs of Reinforcement Learning (RL). Traditional RL methods often use a critic model, which is typically the same size as the policy model, to estimate the value function or baseline. GRPO eliminates the need for this critic model by estimating the baseline from group scores instead.\n\nHereâ€™s how GRPO works:\n\n1. **Group Sampling**: For each question or input \\( q \\), GRPO samples a group of outputs \\( \\{o_1, o_2, \\dots, o_G\\} \\) from the old policy \\( \\pi_{\\theta_{old}} \\).\n2. **Policy Optimization**: The policy model \\( \\pi_{\\theta} \\) is then optimized by maximizing an objective function that leverages the group scores, rather than relying on a separate critic model.\n\nBy avoiding the use of a critic model, GRPO reduces the computational and memory costs associated with training RL models, making the process more efficient.",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}