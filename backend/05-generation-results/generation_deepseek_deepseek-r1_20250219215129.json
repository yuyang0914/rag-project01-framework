{
  "query": "What is GRPO",
  "timestamp": "2025-02-19T21:51:29.391244",
  "provider": "deepseek",
  "model": "deepseek-r1",
  "response": "ã€æ€ç»´è¿‡ç¨‹ã€‘\nOkay, so I need to explain what GRPO is based on the provided context. Let me start by reading the context carefully again.\n\nThe context says that Group Relative Policy Optimization (GRPO) is used to save training costs in RL by not using a critic model. Instead, it estimates the baseline from group scores. For each question, it samples a group of outputs from the old policy and then optimizes the policy model by maximizing a specific objective.\n\nHmm, so GRPO is an optimization method in reinforcement learning. The key points here are that it removes the need for a critic model, which is usually as big as the policy model. That must reduce computational resources or training time. Instead of using a critic, they use group scores. How does that work? The method samples multiple outputs (G of them) from the old policy for each question. Then, using these samples, they estimate the baseline. The baseline in RL helps in reducing variance by comparing the rewards to this baseline. So, GRPO replaces the critic's baseline estimation with an average or some aggregation of the group's scores.\n\nThe objective function they mention is probably similar to other policy optimization methods like PPO or REINFORCE, but with the baseline derived from the group. So instead of having a separate neural network (the critic) to estimate the value function, GRPO uses the average reward of the sampled group as the baseline. That makes sense because the critic model can be large, and eliminating it saves on training costs. \n\nWait, the context says \"estimates the baseline from group scores instead.\" So each output in the group is generated by the old policy, and their scores are used to compute the baseline. For example, maybe the average reward of the group becomes the baseline. Then, when optimizing the policy, they use these baselines to adjust the advantages, which helps in updating the policy more efficiently.\n\nSo putting it all together, GRPO is a variant of policy optimization algorithms that forgoes the critic model by using the average (or some other measure) of a group of sampled outputs to estimate the baseline, thereby reducing the computational resources required since there's no need to train a separate critic network. This would make the training process faster and cheaper while still maintaining effective learning by reducing variance through the group-based baseline.\n\nã€æœ€ç»ˆç­”æ¡ˆã€‘\nGroup Relative Policy Optimization (GRPO) is a reinforcement learning method designed to reduce training costs by eliminating the need for a critic model, which is typically as large as the policy model. Instead of relying on a critic to estimate value functions, GRPO samples a group of outputs \\(\\{o_1, o_2, \\dots, o_G\\}\\) from the old policy \\(\\pi_{\\theta_{old}}\\) for each input question \\(q\\). It then calculates a **baseline directly from the scores of these sampled outputs**, using this group-based baseline to optimize the policy \\(\\pi_\\theta\\). \n\nBy replacing the critic with group score estimation, GRPO avoids the computational and memory overhead of training a separate critic network, streamlining the training process while still enabling effective policy updates through variance reduction via the group-derived baseline. This approach is particularly advantageous in resource-constrained scenarios.",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}