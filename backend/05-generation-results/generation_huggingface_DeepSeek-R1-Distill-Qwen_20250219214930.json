{
  "query": "What is GRPO",
  "timestamp": "2025-02-19T21:49:30.695473",
  "provider": "huggingface",
  "model": "DeepSeek-R1-Distill-Qwen",
  "response": "è¯·åœ¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾é—®é¢˜ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚\n</think>\n\nThe problem is to determine what GRPO is, based on the provided context.\n\nThe context mentions that GRPO stands for Group Relative Policy Optimization. It is used in RL to reduce training costs by avoiding the use of a critic model that is typically the same size as the policy model. Instead, it estimates the baseline from group scores, where it samples a group of outputs from the old policy and optimizes the policy model to maximize a specific objective.\n\nTherefore, GRPO is a method in reinforcement learning that optimizes policies by using group-based sampling to reduce the need for a critic model, thereby saving training costs.",
  "context": [
    {
      "text": "Group Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:",
      "score": 0.5254793167114258,
      "metadata": {
        "source": "DeepSeek_R1.pdf",
        "page": "5",
        "chunk": 41,
        "total_chunks": 180,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-02-14T16:48:03.946618"
      }
    }
  ]
}